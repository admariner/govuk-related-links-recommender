{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging.config\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import pymongo\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from src.utils.miscellaneous import read_config_yaml\n",
    "from src.utils import text_preprocessing as tp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-olive",
   "metadata": {},
   "source": [
    "\n",
    "### Changes for AB test:\n",
    "    - Weighted\n",
    "    - Add taxons and departments and document_collections\n",
    "    - Prune network (remove transitions that have <1% probability of occurring\n",
    "    - Increase dimensions (128)\n",
    "    - Increase batchsize (50)\n",
    "    - Threshold set at 80 rather than the fiddly logic (check how many have wouldnt have links in this scenario)\n",
    "    - Train on aws sagemaker notebook - related-links-large\n",
    "    \n",
    "### Todo\n",
    "    - Taxons and department link extraction only implemented in this notebook (not the main src of the app)\n",
    "    - Network pruning not implemented\n",
    "    - Threshold change only implemented in notebook `run-and-check-link-predictions` (not in src!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client = pymongo.MongoClient('mongodb://localhost:27017/')\n",
    "# TODO check this is consistent with naming of restored db in AWS\n",
    "content_store_db = mongo_client[\"content_store\"]\n",
    "content_store_collection = content_store_db[\"content_items\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='bs4')\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "KEYS_FOR_LINK_TYPES = {\n",
    "    \"related\": \"ordered_related_items\",\n",
    "    \"collection\": \"documents\",\n",
    "    \"taxons\":\"taxons\",\n",
    "    \"documents\": \"documents\",\n",
    "    \"document_collections\":\"document_colletions\",\n",
    "    \"organisations\":\"organisations\"\n",
    "}\n",
    "\n",
    "EXCLUDED_SOURCE_CONTENT = read_config_yaml(\"source_exclusions_that_are_not_linked_from.yml\")\n",
    "EXCLUDED_TARGET_CONTENT = read_config_yaml(\"target_exclusions_that_are_not_linked_to.yml\")\n",
    "\n",
    "LINKS_PROJECTION = {\n",
    "    \"expanded_links.ordered_related_items.base_path\": 1,\n",
    "    \"expanded_links.ordered_related_items.content_id\": 1,\n",
    "    \"expanded_links.taxons.base_path\":1,\n",
    "    \"expanded_links.taxons.content_id\":1,\n",
    "    \"expanded_links.documents.base_path\":1,\n",
    "    \"expanded_links.documents.content_id\":1,\n",
    "    \"expanded_links.document_collections.base_path\":1,\n",
    "    \"expanded_links.document_collections.content_id\":1,\n",
    "    \"expanded_links.organisations.base_path\":1,\n",
    "    \"expanded_links.organisations.content_id\":1,\n",
    "    \"content_id\": 1}\n",
    "\n",
    "\n",
    "TEXT_PROJECTION = {\n",
    "    \"details.body\": 1,\n",
    "    \"details.brand\": 1,  # no documents found?\n",
    "    \"details.documents\": 1,\n",
    "    \"details.final_outcome_detail\": 1,\n",
    "    \"details.final_outcome_documents\": 1,\n",
    "    \"details.government\": 1,\n",
    "    \"details.headers\": 1,\n",
    "    \"details.introduction\": 1,\n",
    "    \"details.introductory_paragraph\": 1,\n",
    "    \"details.licence_overview\": 1,\n",
    "    \"details.licence_short_description\": 1,\n",
    "    \"details.logo\": 1,\n",
    "    \"details.metadata\": 1,\n",
    "    \"details.more_information\": 1,\n",
    "    \"details.need_to_know\": 1,\n",
    "    \"details.other_ways_to_apply\": 1,\n",
    "    \"details.summary\": 1,\n",
    "    \"details.ways_to_respond\": 1,\n",
    "    \"details.what_you_need_to_know\": 1,\n",
    "    \"details.will_continue_on\": 1,\n",
    "    \"details.parts\": 1,\n",
    "    \"details.collection_groups\": 1,\n",
    "    \"details.transaction_start_link\": 1,\n",
    "    \"content_id\": 1}\n",
    "\n",
    "FILTER_BASIC = {\"$and\": [{\"phase\": \"live\"}]}\n",
    "\n",
    "\n",
    "\n",
    "OUTPUT_DF_COLUMNS = ['destination_base_path', 'destination_content_id', 'source_base_path', 'source_content_id']\n",
    "\n",
    "\n",
    "def get_key_links_df(mongodb_collection):\n",
    "    \"\"\"\n",
    "    Gets links as per the links projections\n",
    "    \"\"\"\n",
    "    linklist =  list(content_store_collection.find(FILTER_BASIC, LINKS_PROJECTION))\n",
    "    df = json_normalize(linklist)\n",
    "    df = df.melt(id_vars=['_id','content_id'])\n",
    "    df = df.dropna()\n",
    "    df = reshape_df_explode_list_column(df, 'value')\n",
    "    df['destination_content_id'] = df['value'].map(lambda x: x['content_id'])\n",
    "    df['destination_page_path'] = df['value'].map(lambda x: x['base_path'])\n",
    "    df = df[['_id','content_id','variable','destination_content_id','destination_page_path']]\n",
    "    df['variable'] = df['variable'].map(lambda x: x.split('.')[1])\n",
    "    df.columns = ['source_base_path','source_content_id','link_type','destination_content_id','destination_base_path']\n",
    "    return df\n",
    "\n",
    "def get_page_text_df(mongodb_collection):\n",
    "    \"\"\"\n",
    "    Queries a MongoDB collection, get specific fields from details using TEXT_PROJECTION, converts this cursor to a\n",
    "        DataFrame, with all details fields in one list column\n",
    "    :param mongodb_collection:\n",
    "    :return: pandas DataFrame with: _id (base_path), content_id, and all_details list column\n",
    "    \"\"\"\n",
    "    text_list = list(mongodb_collection.find(FILTER_BASIC, TEXT_PROJECTION))\n",
    "    df = json_normalize(text_list)\n",
    "    # concatenate text from all columns (except first 2) into a list -> so we get a list of all the details fields\n",
    "    # that we queried\n",
    "    df['all_details'] = df.iloc[:, 2:-1].values.tolist()\n",
    "    logging.info(f' df with details text has columns={list(df.columns)} and shape={df.shape}')\n",
    "    return df[['_id', 'content_id', 'all_details']]\n",
    "\n",
    "\n",
    "def reshape_df_explode_list_column(wide_df, list_column):\n",
    "    \"\"\"\n",
    "    Bit like a melt, we have a list column in a DataFrame, and we repeat all other columns for each item in the list\n",
    "    TODO: would be nice to bump pandas and call DataFrame.explode, but it breaks other stuff\n",
    "    :param wide_df: pandas DataFrame with a list column\n",
    "    :param list_column: list column name\n",
    "    :return: DataFrame with one row per item in the list_column\n",
    "    \"\"\"\n",
    "    # repeat all columns except list_col as many times as the list is long for that row\n",
    "    # get a 1D vecotr using concatenate to flatten all values in list vector\n",
    "    # and unpack this vector into a new column called list_col\n",
    "    return pd.DataFrame({\n",
    "        col: np.repeat(wide_df[col].values, wide_df[list_column].str.len())\n",
    "        for col in wide_df.columns.difference([list_column])\n",
    "    }).assign(**{list_column: np.concatenate(wide_df[list_column].values)})[\n",
    "        wide_df.columns.tolist()]\n",
    "\n",
    "\n",
    "def extract_embedded_links_df(page_text_df, base_path_to_content_id_mapping):\n",
    "    \"\"\"\n",
    "    Takes a dataframe with  a list column (all_details), returns a dataframe with one in-page (embedded) link per row\n",
    "    :param page_text_df: pandas DataFrame with  a list column (all_details)\n",
    "    :param base_path_to_content_id_mapping: Python dictionary {page_path: content_id}\n",
    "    :return:  pandas DataFrame  of embedded links with columns ['source_base_path', 'source_content_id',\n",
    "        'destination_base_path','destination_content_id', 'link_type']\n",
    "    \"\"\"\n",
    "    page_text_df['embedded_links'] = page_text_df['all_details'].progress_apply(tp.extract_links_from_content_details)\n",
    "    logging.info(f'have applied extract_links_from_content_details to page_text_df')\n",
    "\n",
    "    embedded_links_df = page_text_df[['_id', 'content_id', 'embedded_links']]\n",
    "    logging.info(f'shape of df with link list (wide before melt)={embedded_links_df.shape}')\n",
    "\n",
    "    embedded_links_df = reshape_df_explode_list_column(embedded_links_df, 'embedded_links')\n",
    "    logging.info(f'shape of df after melt (each link in its own row)={embedded_links_df.shape}')\n",
    "\n",
    "    embedded_links_df['embedded_links'] = embedded_links_df['embedded_links'].apply(tp.clean_page_path)\n",
    "    embedded_links_df['destination_content_id'] = embedded_links_df['embedded_links'].map(\n",
    "        base_path_to_content_id_mapping)\n",
    "    logging.info(f'mapping of page_path to content_id has completed')\n",
    "\n",
    "    embedded_links_df.rename(\n",
    "        columns={\n",
    "            '_id': 'source_base_path',\n",
    "            'content_id': 'source_content_id',\n",
    "            'embedded_links': 'destination_base_path'},\n",
    "        inplace=True)\n",
    "\n",
    "    embedded_links_df['link_type'] = 'embedded_link'\n",
    "    return embedded_links_df\n",
    "\n",
    "\n",
    "def get_structural_edges_df(mongodb_collection, page_path_content_id_mapping):\n",
    "    \"\"\"\n",
    "    Gets related, collection, and embedded links for all items in the mongodb collection\n",
    "    :param mongodb_collection:\n",
    "    :param page_path_content_id_mapping: Python dictionary {page_path: content_id}\n",
    "    :return: pandas DataFrame with columns ['source_base_path', 'source_content_id', 'destination_base_path',\n",
    "                                 'destination_content_id', 'link_type']\n",
    "    \"\"\"\n",
    "    key_links_df = get_key_links_df(mongodb_collection)\n",
    "\n",
    "    page_text_df = get_page_text_df(mongodb_collection)\n",
    "\n",
    "    embedded_links_df = extract_embedded_links_df(page_text_df, page_path_content_id_mapping)\n",
    "    logging.info(f'embedded links dataframe shape {embedded_links_df.shape}')\n",
    "\n",
    "    structural_edges_df = pd.concat(\n",
    "        [key_links_df, embedded_links_df],\n",
    "        axis=0, sort=True, ignore_index=True)\n",
    "\n",
    "    logging.info(f'structural edges dataframe shape {structural_edges_df.shape}')\n",
    "\n",
    "    # filter out any links without a destination content ID, as we are building a network based on content_ids\n",
    "    structural_edges_df.query('destination_content_id.notnull()', inplace=True)\n",
    "    logging.info(\n",
    "        f'structural edges dataframe shape f after dropping null destination_content_ids={structural_edges_df.shape}')\n",
    "    return structural_edges_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-reminder",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_key_links_df(content_store_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-honolulu",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-cookbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "sn = pd.read_csv('../data/tmp/structural_edges.csv')\n",
    "sn = sn[sn['link_type']=='embedded_link']\n",
    "sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-roulette",
   "metadata": {},
   "outputs": [],
   "source": [
    "c['weight'] = 50\n",
    "\n",
    "all_edges = pd.concat([c, f],\n",
    "                          ignore_index=True, sort=True)\n",
    "\n",
    "# Deduplicate edges, summing structural and functional edge weights\n",
    "all_edges = all_edges.groupby(['source_content_id', 'destination_content_id'], as_index=False).aggregate(sum)\n",
    "all_edges = all_edges[\n",
    "        ['source_content_id', 'destination_content_id', 'weight']].reset_index(drop=True)\n",
    "all_edges.to_csv('metanetwork.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-order",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_csv('../data/tmp/functional_edges.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-launch",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.to_csv('metanetwork.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
