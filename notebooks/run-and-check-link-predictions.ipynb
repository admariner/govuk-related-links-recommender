{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from pigeon import annotate\n",
    "import pprint\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from gensim.models import Word2Vec\n",
    "from src.utils.big_query_client import BigQueryClient\n",
    "from src.utils.epoch_logger import EpochLogger\n",
    "from src.utils.miscellaneous import load_pickled_content_id_list\n",
    "from src.utils.related_links_csv_exporter import RelatedLinksCsvExporter\n",
    "from src.utils.related_links_json_exporter import RelatedLinksJsonExporter\n",
    "from src.utils.related_links_predictor import RelatedLinksPredictor\n",
    "from src.utils.related_links_confidence_filter import RelatedLinksConfidenceFilter\n",
    "from src.utils.date_helper import DateHelper\n",
    "from src.utils.miscellaneous import read_config_yaml\n",
    "from src.utils.big_query_client import BigQueryClient\n",
    "from src.utils.date_helper import DateHelper\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-hours",
   "metadata": {},
   "source": [
    "\n",
    "### Changes for AB test:\n",
    "    - Weighted\n",
    "    - Add taxons and departments and document_collections\n",
    "    - Prune network (remove transitions that have <1% probability of occurring\n",
    "    - Increase dimensions (128)\n",
    "    - Increase batchsize (50)\n",
    "    - Threshold set at 80 rather than the fiddly logic (check how many have wouldnt have links in this scenario)\n",
    "    - Train on aws sagemaker notebook - related-links-large\n",
    "    \n",
    "### Todo\n",
    "    - Taxons and department link extraction only implemented in get_all_links notebook (not the main src of the app)\n",
    "    - Network pruning not implemented\n",
    "    - Threshold change only implemented in notebook `run-and-check-link-predictions` (not in src!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_id_to_base_path_mapper(path):\n",
    "    with open(path, 'r') as content_id_to_base_path_mapping_file:\n",
    "        return json.load(content_id_to_base_path_mapping_file)\n",
    "\n",
    "\n",
    "def get_content_ids_to_page_views_mapper(df):\n",
    "    \"\"\"\n",
    "    Transform BigQuery dataframe to a dictionary where keys are content_ids and the values are pageviews.\n",
    "    :param df:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return df.set_index('content_id').T.to_dict('records')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"yesterday = DateHelper.get_datetime_for_yesterday()\n",
    "three_weeks_ago = DateHelper.get_datetime_for_days_ago(40)\n",
    "\n",
    "bq_client = BigQueryClient()\n",
    "query_path = '../src/models/query_eligible_source_content_ids.sql'\n",
    "all_content_ids_and_views_df = bq_client.query_date_range(query_path, three_weeks_ago, yesterday)\n",
    "all_content_ids_and_views_df.to_csv('../data/tmp/all_page_views.csv', index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-strength",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_content_ids_and_views_df = pd.read_csv('../data/tmp/all_pages_views.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_content_ids_and_views_df = all_content_ids_and_views_df[['content_id','page_hits']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-digit",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_content_ids_and_views_df.columns = ['target_content_id','hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_ids = get_content_id_to_base_path_mapper('content_id_base_path_mapping.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-exclusive",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/tmp/'\n",
    "MODEL_DIR = '../models/t/'\n",
    "model_name = 'lite_weighted_n2v'\n",
    "predictions1 = 'n2vweightedtest'\n",
    "\n",
    "related_links_path = os.path.join(DATA_DIR,   predictions1 +\n",
    "                                  datetime.today().strftime('%Y%m%d') + \"suggested_related_links\")\n",
    "\n",
    "\n",
    "content_id_base_mapping_path = '../data/tmp/content_id_base_path_mapping.json'\n",
    "\n",
    "node2vec_model_file_path = os.path.join(MODEL_DIR, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-patio",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = Word2Vec.load(node2vec_model_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run on a sample of eligible bits of content\n",
    "\n",
    "eligible_source_content_ids = load_pickled_content_id_list(os.path.join(DATA_DIR,\n",
    "                                                                        \"eligible_source_content_ids.pkl\"))\n",
    "\n",
    "eligible_target_content_ids = load_pickled_content_id_list(os.path.join(DATA_DIR,\n",
    "                                                                        \"eligible_target_content_ids.pkl\"))\n",
    "\n",
    "eligible_target_content_ids = set(eligible_target_content_ids)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-angola",
   "metadata": {},
   "outputs": [],
   "source": [
    "eligible_source_content_ids = list(set(eligible_source_content_ids))\n",
    "len(eligible_source_content_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-spirituality",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_content_ids_and_views_df = pd.read_csv('../data/tmp/all_pages_views.csv')\n",
    "all_content_ids_and_views_df = all_content_ids_and_views_df[['content_id','page_hits']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(eligible_source_content_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from multiprocessing import cpu_count\n",
    "import numpy as np\n",
    "import logging.config\n",
    "import os\n",
    "from collections import ChainMap\n",
    "\n",
    "class RelatedLinksPredictor:\n",
    "    \"\"\"\n",
    "    Uses a node2vec model to create a nested list of source_content_ids and their predicted target_content_ids (up to 5)\n",
    "    :param source_content_ids: list of content_ids we can link from\n",
    "    :param target_content_ids: list of content_ids we can link to\n",
    "    :param model: node2vec model where model.wv.vocab.keys() are content_ids\n",
    "    :param probability_threshold: number in the range [0,1] representing the similarity of two nodes.\n",
    "   :param num_links: maximum number of links to recommend (optional)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, source_content_ids, target_content_ids, model, num_links=5):\n",
    "        self.model = model\n",
    "        self.eligible_source_content_ids = self._get_eligible_content_ids(source_content_ids)\n",
    "        self.eligible_target_content_ids = target_content_ids\n",
    "        self.num_links = num_links\n",
    "\n",
    "\n",
    "    def predict_all_related_links(self, num_workers=cpu_count()):\n",
    "        params = list(map(\n",
    "            lambda source_content_id: (\n",
    "                source_content_id, self.eligible_target_content_ids, self.model,\n",
    "                self.num_links),\n",
    "            self._split_content_ids(self.eligible_source_content_ids, num_workers)))\n",
    "\n",
    "        pool = multiprocessing.Pool(processes=num_workers)\n",
    "        results = pool.starmap(_predict_related_links_for_content_ids, params)\n",
    "\n",
    "        all_related_links = dict(ChainMap(*results))\n",
    "\n",
    "        pool.close()\n",
    "\n",
    "        return all_related_links\n",
    "\n",
    "    def _get_eligible_content_ids(self, source_content_ids):\n",
    "        \"\"\"\n",
    "        Filter eligible content_ids to only the ones included in the trained model's vocabulary\n",
    "        :param source_content_ids:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        return [\n",
    "            content_id for content_id in tqdm(\n",
    "                source_content_ids, desc=\"eligible_content_ids\"\n",
    "            ) if content_id in self.model.wv.vocab.keys()\n",
    "        ]\n",
    "\n",
    "    def _split_content_ids(self, content_ids, chunks):\n",
    "        \"\"\"\n",
    "\n",
    "        :param content_ids:\n",
    "        :param chunks:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return np.array_split(content_ids, chunks)\n",
    "\n",
    "\n",
    "\n",
    "def _predict_related_links_for_content_ids(source_content_ids, eligible_target_content_ids, model,\n",
    "                                            num_links):\n",
    "    \"\"\"\n",
    "    Gets the top-5 most-probable eligible target_content_ids for a single source_content_id.\n",
    "    Target_content_ids are dropped if:\n",
    "        - The predicted probability between source and target is below the probability threshold\n",
    "        - The target_content_id is not listed in the inclusion list\n",
    "        - The source and target are the same item\n",
    "        - The link is not in the top 5 (highest probabilities) for that source_id\n",
    "    \"\"\"\n",
    "\n",
    "    related_links = {}\n",
    "\n",
    "    print(f\"Computing related links for {len(source_content_ids)} content_ids, worker id: {os.getpid()}\")\n",
    "\n",
    "    for content_id in tqdm(source_content_ids, desc=\"getting related links\"):\n",
    "        # stick to this approach because actually interacting with the most_similar generator is\n",
    "        # super slow. Dump everything to a dataframe, then filter and save list values\n",
    "        potential_related_links = pd.DataFrame(model.wv.most_similar(content_id, topn=100))\n",
    "        potential_related_links.columns = ['target_content_id', 'probability']\n",
    "        potential_related_links['source_content_id'] = content_id\n",
    "        \n",
    "        mask = potential_related_links['target_content_id'].map(lambda x: x in eligible_target_content_ids)\n",
    "        \n",
    "        potential_related_links = potential_related_links[mask]\n",
    "        \n",
    "        potential_related_links = potential_related_links[potential_related_links['probability']>0.8]\n",
    "        \n",
    "        if potential_related_links.shape[0] ==0:\n",
    "            related_links[content_id] = [] \n",
    "            continue\n",
    "    \n",
    "        potential_related_links= potential_related_links.sort_values('probability',ascending=False).head(5)\n",
    "\n",
    "        related_links[content_id] = potential_related_links[['target_content_id','probability']].values.tolist()\n",
    "\n",
    "    return related_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-front",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "related_links_predictor = RelatedLinksPredictor(eligible_source_content_ids, eligible_target_content_ids,trained_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_id_base_mapping_path = 'content_id_base_path_mapping.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "related_links = related_links_predictor.predict_all_related_links()\n",
    "\n",
    "json_exporter = RelatedLinksJsonExporter(related_links)\n",
    "json_exporter.export(f'{related_links_path}.json')\n",
    "\n",
    "\n",
    "csv_exporter = RelatedLinksCsvExporter(related_links,\n",
    "                                       get_content_id_to_base_path_mapper(content_id_base_mapping_path),\n",
    "                                       get_content_ids_to_page_views_mapper(all_content_ids_and_views_df))\n",
    "\n",
    "csv_exporter.export(f'{related_links_path}.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-dominican",
   "metadata": {},
   "outputs": [],
   "source": [
    "pagepath_related_links = {}\n",
    "for source_id in related_links:\n",
    "    link_ids = [target[0] for target in related_links[source_id]]\n",
    "    link_paths = [os.path.splitext(content_ids[target_id])[0] for target_id in link_ids]\n",
    "    sourcepath = os.path.splitext(content_ids[source_id])[0]\n",
    "    pagepath_related_links[sourcepath] = link_paths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-windows",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pagepath_related_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-injection",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'random_related_links_all.yml', 'w') as file:\n",
    "    yaml.dump(pagepath_related_links, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-incidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "recent = pd.read_csv('recent_related_links.csv')\n",
    "recent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "recent = recent[['source_content_id',\n",
    "           'destination_base_path',\n",
    "           ]].groupby(['source_content_id',]).aggregate(list).reset_index()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-thread",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted = pd.read_csv(related_links_path +'.tsv',sep='\\t')\n",
    "weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted = pd.read_csv(related_links_path +'.tsv',sep='\\t')\n",
    "weighted = weighted[['source_content_id',\n",
    "                     'source_base_path',\n",
    "                     'target_base_path',\n",
    "                     'target_content_id',\n",
    "                     'source_page_views']].groupby(['source_content_id',\n",
    "                                                    'source_base_path',\n",
    "                                                    'source_page_views']).aggregate(list).reset_index()\n",
    "\n",
    "weighted = weighted.replace(np.nan, 'none')\n",
    "weighted = weighted.fillna('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_colwidth\", 400)\n",
    "\n",
    "weighted.sort_values('source_page_views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = weighted.merge(recent, how='left', on='source_content_id')\n",
    "combined = combined.replace(np.nan, 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-physiology",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-indie",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_exporter.export(f'{related_links_path}.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
